--- ./drbd_int.h
+++ /tmp/cocci-output-201565-eafbdc-drbd_int.h
@@ -390,7 +390,7 @@ struct drbd_peer_request {
 	struct list_head wait_for_actlog;
 
 	struct drbd_page_chain_head page_chain;
-	unsigned int opf; /* to be used as bi_opf */
+	unsigned int rw; /* to be used as bi_opf */
 	atomic_t pending_bios;
 	struct drbd_interval i;
 	unsigned long flags;	/* see comments on ee flag bits below */
@@ -413,10 +413,6 @@ struct drbd_peer_request {
 	};
 };
 
-/* Equivalent to bio_op and req_op. */
-#define peer_req_op(peer_req) \
-	((peer_req)->opf & REQ_OP_MASK)
-
 /* ee flag bits.
  * While corresponding bios are in flight, the only modification will be
  * set_bit WAS_ERROR, which has to be atomic.
@@ -1772,8 +1768,8 @@ extern struct kmem_cache *drbd_request_c
 extern struct kmem_cache *drbd_ee_cache;	/* peer requests */
 extern struct kmem_cache *drbd_bm_ext_cache;	/* bitmap extents */
 extern struct kmem_cache *drbd_al_ext_cache;	/* activity log extents */
-extern mempool_t drbd_request_mempool;
-extern mempool_t drbd_ee_mempool;
+extern mempool_t *drbd_request_mempool;
+extern mempool_t *drbd_ee_mempool;
 
 /* drbd's page pool, used to buffer data received from the peer,
  * or data requested by the peer.
@@ -1799,16 +1795,16 @@ extern wait_queue_head_t drbd_pp_wait;
  * 128 should be plenty, currently we probably can get away with as few as 1.
  */
 #define DRBD_MIN_POOL_PAGES	128
-extern mempool_t drbd_md_io_page_pool;
+extern mempool_t *drbd_md_io_page_pool;
 
 /* We also need to make sure we get a bio
  * when we need it for housekeeping purposes */
-extern struct bio_set drbd_md_io_bio_set;
+extern struct bio_set * drbd_md_io_bio_set;
 /* to allocate from that set */
 extern struct bio *bio_alloc_drbd(gfp_t gfp_mask);
 
 /* And a bio_set for cloning */
-extern struct bio_set drbd_io_bio_set;
+extern struct bio_set * drbd_io_bio_set;
 
 extern struct drbd_peer_device *create_peer_device(struct drbd_device *, struct drbd_connection *);
 extern enum drbd_ret_code drbd_create_device(struct drbd_config_context *adm_ctx, unsigned int minor,
@@ -1833,13 +1829,15 @@ extern void drbd_transport_shutdown(stru
 extern void drbd_destroy_connection(struct kref *kref);
 extern void conn_free_crypto(struct drbd_connection *connection);
 
+extern int drbd_merge_bvec(struct request_queue *, struct bvec_merge_data *,
+			   struct bio_vec *);
 /* drbd_req */
 extern void do_submit(struct work_struct *ws);
 #ifndef CONFIG_DRBD_TIMING_STATS
 #define __drbd_make_request(d,b,k,j) __drbd_make_request(d,b,j)
 #endif
 extern void __drbd_make_request(struct drbd_device *, struct bio *, ktime_t, unsigned long);
-extern blk_qc_t drbd_make_request(struct request_queue *q, struct bio *bio);
+extern void drbd_make_request(struct request_queue *q, struct bio *bio);
 
 /* drbd_nl.c */
 enum suspend_scope {
@@ -1893,7 +1891,7 @@ extern void verify_progress(struct drbd_
 extern void *drbd_md_get_buffer(struct drbd_device *device, const char *intent);
 extern void drbd_md_put_buffer(struct drbd_device *device);
 extern int drbd_md_sync_page_io(struct drbd_device *device,
-		struct drbd_backing_dev *bdev, sector_t sector, int op);
+		struct drbd_backing_dev *bdev, sector_t sector, int rw);
 extern void drbd_ov_out_of_sync_found(struct drbd_peer_device *, sector_t, int);
 extern void wait_until_done_or_force_detached(struct drbd_device *device,
 		struct drbd_backing_dev *bdev, unsigned int *done);
@@ -1902,7 +1900,7 @@ extern void drbd_check_peers(struct drbd
 extern void drbd_check_peers_new_current_uuid(struct drbd_device *);
 extern void drbd_ping_peer(struct drbd_connection *connection);
 extern struct drbd_peer_device *peer_device_by_node_id(struct drbd_device *, int);
-extern void repost_up_to_date_fn(struct timer_list *t);
+extern void repost_up_to_date_fn(unsigned long data);
 
 static inline void ov_out_of_sync_print(struct drbd_peer_device *peer_device)
 {
@@ -1940,15 +1938,15 @@ extern int w_restart_disk_io(struct drbd
 extern int w_start_resync(struct drbd_work *, int);
 extern int w_send_uuids(struct drbd_work *, int);
 
-extern void resync_timer_fn(struct timer_list *t);
-extern void start_resync_timer_fn(struct timer_list *t);
+extern void resync_timer_fn(unsigned long data);
+extern void start_resync_timer_fn(unsigned long data);
 
 extern void drbd_endio_write_sec_final(struct drbd_peer_request *peer_req);
 
 /* bi_end_io handlers */
-extern void drbd_md_endio(struct bio *bio);
-extern void drbd_peer_request_endio(struct bio *bio);
-extern void drbd_request_endio(struct bio *bio);
+extern void drbd_md_endio(struct bio *bio, int error);
+extern void drbd_peer_request_endio(struct bio *bio, int error);
+extern void drbd_request_endio(struct bio *bio, int error);
 
 void __update_timing_details(
 		struct drbd_thread_timing_details *tdp,
@@ -2027,7 +2025,7 @@ extern void apply_unacked_peer_requests(
 extern struct drbd_connection *drbd_connection_by_node_id(struct drbd_resource *, int);
 extern struct drbd_connection *drbd_get_connection_by_node_id(struct drbd_resource *, int);
 extern void queue_queued_twopc(struct drbd_resource *resource);
-extern void queued_twopc_timer_fn(struct timer_list *t);
+extern void queued_twopc_timer_fn(unsigned long data);
 extern bool drbd_have_local_disk(struct drbd_resource *resource);
 extern enum drbd_state_rv drbd_support_2pc_resize(struct drbd_resource *resource);
 extern enum determine_dev_size
@@ -2061,8 +2059,8 @@ static inline void drbd_generic_make_req
 	__release(local);
 
 	if (drbd_insert_fault(device, fault_type)) {
-		bio->bi_status = BLK_STS_IOERR;
-		bio_endio(bio);
+		bio_endio(bio,
+			  (10 == 0 ? 0 : 10 == 9 ? -ENOMEM : 10 == 1 ? -EOPNOTSUPP : -EIO));
 	} else {
 		generic_make_request(bio);
 	}
@@ -2071,8 +2069,8 @@ static inline void drbd_generic_make_req
 void drbd_bump_write_ordering(struct drbd_resource *resource, struct drbd_backing_dev *bdev,
 			      enum write_ordering_e wo);
 
-extern void twopc_timer_fn(struct timer_list *t);
-extern void connect_timer_fn(struct timer_list *t);
+extern void twopc_timer_fn(unsigned long data);
+extern void connect_timer_fn(unsigned long data);
 
 /* drbd_proc.c */
 extern struct proc_dir_entry *drbd_proc;
--- ./drbd_req.h
+++ /tmp/cocci-output-201565-74d498-drbd_req.h
@@ -295,7 +295,7 @@ extern void __req_mod(struct drbd_reques
 		struct bio_and_error *m);
 extern void complete_master_bio(struct drbd_device *device,
 		struct bio_and_error *m);
-extern void request_timer_fn(struct timer_list *t);
+extern void request_timer_fn(unsigned long data);
 extern void tl_walk(struct drbd_connection *connection, enum drbd_req_event what);
 extern void _tl_walk(struct drbd_connection *connection, enum drbd_req_event what);
 extern void __tl_walk(struct drbd_resource *const resource,
--- drbd_bitmap.c
+++ /tmp/cocci-output-201565-03f6c9-drbd_bitmap.c
@@ -1081,14 +1081,14 @@ static void drbd_bm_aio_ctx_destroy(stru
 }
 
 /* bv_page may be a copy, or may be the original */
-static void drbd_bm_endio(struct bio *bio)
+static void drbd_bm_endio(struct bio *bio, int error)
 {
 	struct drbd_bm_aio_ctx *ctx = bio->bi_private;
 	struct drbd_device *device = ctx->device;
 	struct drbd_bitmap *b = device->bitmap;
 	unsigned int idx = bm_page_to_idx(bio->bi_io_vec[0].bv_page);
 
-	blk_status_t status = bio->bi_status;
+	u8 status = (error == 0 ? 0 : error == -ENOMEM ? 9 : error == -EOPNOTSUPP ? 1 : 10);
 
 	if ((ctx->flags & BM_AIO_COPY_PAGES) == 0 &&
 	    !bm_test_page_unchanged(b->bm_pages[idx]))
@@ -1097,7 +1097,7 @@ static void drbd_bm_endio(struct bio *bi
 	if (status) {
 		/* ctx error will hold the completed-last non-zero error code,
 		 * in case error codes differ. */
-		ctx->error = blk_status_to_errno(status);
+		ctx->error = (status == 0 ? 0 : status == 9 ? -ENOMEM : status == 1 ? -EOPNOTSUPP : -EIO);
 		bm_set_page_io_err(b->bm_pages[idx]);
 		/* Not identical to on disk version of it.
 		 * Is BM_PAGE_IO_ERROR enough? */
@@ -1112,7 +1112,7 @@ static void drbd_bm_endio(struct bio *bi
 	bm_page_unlock_io(device, idx);
 
 	if (ctx->flags & BM_AIO_COPY_PAGES)
-		mempool_free(bio->bi_io_vec[0].bv_page, &drbd_md_io_page_pool);
+		mempool_free(bio->bi_io_vec[0].bv_page, drbd_md_io_page_pool);
 
 	bio_put(bio);
 
@@ -1130,7 +1130,7 @@ static void bm_page_io_async(struct drbd
 	struct drbd_bitmap *b = device->bitmap;
 	struct page *page;
 	unsigned int len;
-	unsigned int op = (ctx->flags & BM_AIO_READ) ? REQ_OP_READ : REQ_OP_WRITE;
+	unsigned int rw = (ctx->flags & BM_AIO_READ) ? 0 : REQ_WRITE;
 
 	sector_t on_disk_sector =
 		device->ldev->md.md_offset + device->ldev->md.bm_offset;
@@ -1149,26 +1149,26 @@ static void bm_page_io_async(struct drbd
 	bm_set_page_unchanged(b->bm_pages[page_nr]);
 
 	if (ctx->flags & BM_AIO_COPY_PAGES) {
-		page = mempool_alloc(&drbd_md_io_page_pool,
-				GFP_NOIO | __GFP_HIGHMEM);
+		page = mempool_alloc(drbd_md_io_page_pool,
+				     GFP_NOIO | __GFP_HIGHMEM);
 		copy_highpage(page, b->bm_pages[page_nr]);
 		bm_store_page_idx(page, page_nr);
 	} else
 		page = b->bm_pages[page_nr];
-	bio_set_dev(bio, device->ldev->md_bdev);
-	bio->bi_iter.bi_sector = on_disk_sector;
+	bio->bi_bdev = device->ldev->md_bdev;
+	bio->bi_sector = on_disk_sector;
 	/* bio_add_page of a single page to an empty bio will always succeed,
 	 * according to api.  Do we want to assert that? */
 	bio_add_page(bio, page, len, 0);
 	bio->bi_private = ctx;
 	bio->bi_end_io = drbd_bm_endio;
-	bio->bi_opf = op;
+	bio->bi_rw = rw;
 
-	if (drbd_insert_fault(device, (op == REQ_OP_WRITE) ? DRBD_FAULT_MD_WR : DRBD_FAULT_MD_RD)) {
-		bio->bi_status = BLK_STS_IOERR;
-		bio_endio(bio);
+	if (drbd_insert_fault(device, (rw & REQ_WRITE) ? DRBD_FAULT_MD_WR : DRBD_FAULT_MD_RD)) {
+		bio_endio(bio,
+			  (10 == 0 ? 0 : 10 == 9 ? -ENOMEM : 10 == 1 ? -EOPNOTSUPP : -EIO));
 	} else {
-		submit_bio(bio);
+		submit_bio(rw, bio);
 		/* this should not count as user activity and cause the
 		 * resync to throttle -- see drbd_rs_should_slow_down(). */
 		atomic_add(len >> 9, &device->rs_sect_ev);
--- drbd_sender.c
+++ /tmp/cocci-output-201565-53b944-drbd_sender.c
@@ -22,8 +22,6 @@
 #include <linux/slab.h>
 #include <linux/random.h>
 #include <linux/scatterlist.h>
-#include <linux/overflow.h>
-#include <linux/part_stat.h>
 
 #include "drbd_int.h"
 #include "drbd_protocol.h"
@@ -55,14 +53,14 @@ struct mutex resources_mutex;
 /* used for synchronous meta data and bitmap IO
  * submitted by drbd_md_sync_page_io()
  */
-void drbd_md_endio(struct bio *bio)
+void drbd_md_endio(struct bio *bio, int error)
 {
 	struct drbd_device *device;
 
-	blk_status_t status = bio->bi_status;
+	u8 status = (error == 0 ? 0 : error == -ENOMEM ? 9 : error == -EOPNOTSUPP ? 1 : 10);
 
 	device = bio->bi_private;
-	device->md_io.error = blk_status_to_errno(status);
+	device->md_io.error = (status == 0 ? 0 : status == 9 ? -ENOMEM : status == 1 ? -EOPNOTSUPP : -EIO);
 
 	/* special case: drbd_md_read() during drbd_adm_attach() */
 	if (device->ldev)
@@ -197,15 +195,15 @@ void drbd_endio_write_sec_final(struct d
 /* writes on behalf of the partner, or resync writes,
  * "submitted" by the receiver.
  */
-void drbd_peer_request_endio(struct bio *bio)
+void drbd_peer_request_endio(struct bio *bio, int error)
 {
 	struct drbd_peer_request *peer_req = bio->bi_private;
 	struct drbd_device *device = peer_req->peer_device->device;
 	bool is_write = bio_data_dir(bio) == WRITE;
-	bool is_discard = bio_op(bio) == REQ_OP_WRITE_ZEROES ||
-			  bio_op(bio) == REQ_OP_DISCARD;
+	bool is_discard = (false)/* WRITE_ZEROES not supported on this kernel */ ||
+			  (bio->bi_rw & REQ_DISCARD);
 
-	blk_status_t status = bio->bi_status;
+	u8 status = (error == 0 ? 0 : error == -ENOMEM ? 9 : error == -EOPNOTSUPP ? 1 : 10);
 
 	if (status && drbd_ratelimit())
 		drbd_warn(device, "%s: error=%d s=%llus\n",
@@ -235,7 +233,7 @@ void drbd_panic_after_delayed_completion
 
 /* read, readA or write requests on R_PRIMARY coming from drbd_make_request
  */
-void drbd_request_endio(struct bio *bio)
+void drbd_request_endio(struct bio *bio, int error)
 {
 	unsigned long flags;
 	struct drbd_request *req = bio->bi_private;
@@ -243,7 +241,7 @@ void drbd_request_endio(struct bio *bio)
 	struct bio_and_error m;
 	enum drbd_req_event what;
 
-	blk_status_t status = bio->bi_status;
+	u8 status = (error == 0 ? 0 : error == -ENOMEM ? 9 : error == -EOPNOTSUPP ? 1 : 10);
 
 	/* If this request was aborted locally before,
 	 * but now was completed "successfully",
@@ -283,14 +281,13 @@ void drbd_request_endio(struct bio *bio)
 
 	/* to avoid recursion in __req_mod */
 	if (unlikely(status)) {
-		unsigned int op = bio_op(bio);
-		if (op == REQ_OP_DISCARD || op == REQ_OP_WRITE_ZEROES) {
-			if (status == BLK_STS_NOTSUPP)
+		if ((bio->bi_rw & REQ_DISCARD) || (false)/* WRITE_ZEROES not supported on this kernel */) {
+			if (status == 1)
 				what = DISCARD_COMPLETED_NOTSUPP;
 			else
 				what = DISCARD_COMPLETED_WITH_ERROR;
-		} else if (op == REQ_OP_READ) {
-			if (bio->bi_opf & REQ_RAHEAD)
+		} else if (!(bio->bi_rw & REQ_WRITE)) {
+			if (bio->bi_rw & REQ_RAHEAD)
 				what = READ_AHEAD_COMPLETED_WITH_ERROR;
 			else
 				what = READ_COMPLETED_WITH_ERROR;
@@ -302,7 +299,7 @@ void drbd_request_endio(struct bio *bio)
 	}
 
 	bio_put(req->private_bio);
-	req->private_bio = ERR_PTR(blk_status_to_errno(status));
+	req->private_bio = ERR_PTR((status == 0 ? 0 : status == 9 ? -ENOMEM : status == 1 ? -EOPNOTSUPP : -EIO));
 
 	/* not req_mod(), we need irqsave here! */
 	spin_lock_irqsave(&device->resource->req_lock, flags);
@@ -338,8 +335,8 @@ void drbd_csum_pages(struct crypto_shash
 void drbd_csum_bio(struct crypto_shash *tfm, struct bio *bio, void *digest)
 /* kmap compat: KM_USER1 */
 {
-	struct bio_vec bvec;
-	struct bvec_iter iter;
+	struct bio_vec *bvec;
+	int iter;
 	SHASH_DESC_ON_STACK(desc, tfm);
 
 	desc->tfm = tfm;
@@ -348,12 +345,12 @@ void drbd_csum_bio(struct crypto_shash *
 
 	bio_for_each_segment(bvec, bio, iter) {
 		u8 *src;
-		src = kmap_atomic(bvec.bv_page);
-		crypto_shash_update(desc, src + bvec.bv_offset, bvec.bv_len);
+		src = kmap_atomic(bvec->bv_page);
+		crypto_shash_update(desc, src + bvec->bv_offset, bvec->bv_len);
 		kunmap_atomic(src);
 		/* WRITE_SAME has only one segment,
 		 * checksum the payload only once. */
-		if (bio_op(bio) == REQ_OP_WRITE_SAME)
+		if ((bio->bi_rw & REQ_WRITE_SAME))
 			break;
 	}
 	crypto_shash_final(desc, digest);
@@ -425,7 +422,7 @@ static int read_for_csum(struct drbd_pee
 	peer_req->block_id = ID_SYNCER; /* unused */
 
 	peer_req->w.cb = w_e_send_csum;
-	peer_req->opf = REQ_OP_READ;
+	peer_req->rw =0;
 	spin_lock_irq(&device->resource->req_lock);
 	list_add_tail(&peer_req->w.list, &peer_device->connection->read_ee);
 	spin_unlock_irq(&device->resource->req_lock);
@@ -487,9 +484,9 @@ int w_send_uuids(struct drbd_work *w, in
 	return 0;
 }
 
-void resync_timer_fn(struct timer_list *t)
+void resync_timer_fn(unsigned long data)
 {
-	struct drbd_peer_device *peer_device = from_timer(peer_device, t, resync_timer);
+	struct drbd_peer_device *peer_device = (struct drbd_peer_device *)data;
 
 	if (test_bit(SYNC_TARGET_TO_BEHIND, &peer_device->flags))
 		return;
@@ -532,7 +529,7 @@ struct fifo_buffer *fifo_alloc(unsigned
 {
 	struct fifo_buffer *fb;
 
-	fb = kzalloc(struct_size(fb, values, fifo_size), GFP_NOIO);
+	fb = kzalloc(sizeof(*fb) + sizeof(*fb->values) * fifo_size, GFP_NOIO);
 	if (!fb)
 		return NULL;
 
@@ -1889,9 +1886,9 @@ void drbd_rs_controller_reset(struct drb
 	rcu_read_unlock();
 }
 
-void start_resync_timer_fn(struct timer_list *t)
+void start_resync_timer_fn(unsigned long data)
 {
-	struct drbd_peer_device *peer_device = from_timer(peer_device, t, start_resync_timer);
+	struct drbd_peer_device *peer_device = (struct drbd_peer_device *)data;
 	drbd_peer_device_post_work(peer_device, RS_START);
 }
 
@@ -2263,9 +2260,9 @@ static int do_md_sync(struct drbd_device
 	return 0;
 }
 
-void repost_up_to_date_fn(struct timer_list *t)
+void repost_up_to_date_fn(unsigned long data)
 {
-	struct drbd_resource *resource = from_timer(resource, t, repost_up_to_date_timer);
+	struct drbd_resource *resource = (struct drbd_resource *)data;
 	drbd_post_work(resource, TRY_BECOME_UP_TO_DATE);
 }
 
--- drbd_transport_tcp.c
+++ /tmp/cocci-output-201565-08d808-drbd_transport_tcp.c
@@ -429,7 +429,8 @@ static int dtt_try_connect(struct drbd_t
 	peer_addr = path->path.peer_addr;
 
 	what = "sock_create_kern";
-	err = sock_create_kern(&init_net, my_addr.ss_family, SOCK_STREAM, IPPROTO_TCP, &socket);
+	err = sock_create_kern(my_addr.ss_family, SOCK_STREAM, IPPROTO_TCP,
+			       &socket);
 	if (err < 0) {
 		socket = NULL;
 		goto out;
@@ -637,6 +638,7 @@ retry:
 		list_del(&socket_c->list);
 		kfree(socket_c);
 	} else if (listener->listener.pending_accepts > 0) {
+		int ___addr_len;
 		listener->listener.pending_accepts--;
 		spin_unlock_bh(&listener->listener.waiters_lock);
 
@@ -649,7 +651,8 @@ retry:
 		   from the listening socket. */
 		unregister_state_change(s_estab->sk, listener);
 
-		s_estab->ops->getname(s_estab, (struct sockaddr *)&peer_addr, 2);
+		s_estab->ops->getname(s_estab, (struct sockaddr *)&peer_addr,
+				      &___addr_len, 2);
 
 		spin_lock_bh(&listener->listener.waiters_lock);
 		drbd_path2 = drbd_find_path_by_addr(&listener->listener, &peer_addr);
@@ -785,7 +788,8 @@ static int dtt_init_listener(struct drbd
 
 	my_addr = *(struct sockaddr_storage *)addr;
 
-	err = sock_create_kern(&init_net, my_addr.ss_family, SOCK_STREAM, IPPROTO_TCP, &s_listen);
+	err = sock_create_kern(my_addr.ss_family, SOCK_STREAM, IPPROTO_TCP,
+			       &s_listen);
 	if (err) {
 		s_listen = NULL;
 		what = "sock_create_kern";
@@ -1193,19 +1197,19 @@ static int dtt_send_page(struct drbd_tra
 
 static int dtt_send_zc_bio(struct drbd_transport *transport, struct bio *bio)
 {
-	struct bio_vec bvec;
-	struct bvec_iter iter;
+	struct bio_vec *bvec;
+	int iter;
 
 	bio_for_each_segment(bvec, bio, iter) {
 		int err;
 
-		err = dtt_send_page(transport, DATA_STREAM, bvec.bv_page,
-				      bvec.bv_offset, bvec.bv_len,
-				      bio_iter_last(bvec, iter) ? 0 : MSG_MORE);
+		err = dtt_send_page(transport, DATA_STREAM, bvec->bv_page,
+				      bvec->bv_offset, bvec->bv_len,
+				      ((iter) == bio->bi_vcnt - 1) ? 0 : MSG_MORE);
 		if (err)
 			return err;
 
-		if (bio_op(bio) == REQ_OP_WRITE_SAME)
+		if ((bio->bi_rw & REQ_WRITE_SAME))
 			break;
 	}
 	return 0;
--- drbd_receiver.c
+++ /tmp/cocci-output-201565-1f726a-drbd_receiver.c
@@ -32,7 +32,6 @@
 #include <linux/random.h>
 #include <net/ipv6.h>
 #include <linux/scatterlist.h>
-#include <linux/part_stat.h>
 
 #include "drbd_int.h"
 #include "drbd_protocol.h"
@@ -498,7 +497,7 @@ drbd_alloc_peer_req(struct drbd_peer_dev
 	if (drbd_insert_fault(device, DRBD_FAULT_AL_EE))
 		return NULL;
 
-	peer_req = mempool_alloc(&drbd_ee_mempool, gfp_mask & ~__GFP_HIGHMEM);
+	peer_req = mempool_alloc(drbd_ee_mempool, gfp_mask & ~__GFP_HIGHMEM);
 	if (!peer_req) {
 		if (!(gfp_mask & __GFP_NOWARN))
 			drbd_err(device, "%s: allocation failed\n", __func__);
@@ -526,7 +525,7 @@ void __drbd_free_peer_req(struct drbd_pe
 	D_ASSERT(peer_device, atomic_read(&peer_req->pending_bios) == 0);
 	D_ASSERT(peer_device, drbd_interval_empty(&peer_req->i));
 	drbd_free_page_chain(&peer_device->connection->transport, &peer_req->page_chain, is_net);
-	mempool_free(peer_req, &drbd_ee_mempool);
+	mempool_free(peer_req, drbd_ee_mempool);
 }
 
 int drbd_free_peer_reqs(struct drbd_resource *resource, struct list_head *list, bool is_net_ee)
@@ -734,9 +733,9 @@ static bool initial_states_received(stru
 	return rv;
 }
 
-void connect_timer_fn(struct timer_list *t)
+void connect_timer_fn(unsigned long data)
 {
-	struct drbd_connection *connection = from_timer(connection, t, connect_timer);
+	struct drbd_connection *connection = (struct drbd_connection *)data;
 	struct drbd_resource *resource = connection->resource;
 	unsigned long irq_flags;
 
@@ -1093,16 +1092,16 @@ struct one_flush_context {
 	struct issue_flush_context *ctx;
 };
 
-static void one_flush_endio(struct bio *bio)
+static void one_flush_endio(struct bio *bio, int error)
 {
 	struct one_flush_context *octx = bio->bi_private;
 	struct drbd_device *device = octx->device;
 	struct issue_flush_context *ctx = octx->ctx;
 
-	blk_status_t status = bio->bi_status;
+	u8 status = (error == 0 ? 0 : error == -ENOMEM ? 9 : error == -EOPNOTSUPP ? 1 : 10);
 
 	if (status) {
-		ctx->error = blk_status_to_errno(status);
+		ctx->error = (status == 0 ? 0 : status == 9 ? -ENOMEM : status == 1 ? -EOPNOTSUPP : -EIO);
 		drbd_info(device, "local disk FLUSH FAILED with status %d\n", status);
 	}
 	kfree(octx);
@@ -1139,15 +1138,14 @@ static void submit_one_flush(struct drbd
 
 	octx->device = device;
 	octx->ctx = ctx;
-	bio_set_dev(bio, device->ldev->backing_bdev);
+	bio->bi_bdev = device->ldev->backing_bdev;
 	bio->bi_private = octx;
 	bio->bi_end_io = one_flush_endio;
 
 	device->flush_jif = jiffies;
 	set_bit(FLUSH_PENDING, &device->flags);
 	atomic_inc(&ctx->pending);
-	bio->bi_opf = REQ_OP_FLUSH | REQ_PREFLUSH;
-	submit_bio(bio);
+	submit_bio(WRITE_FLUSH, bio);
 }
 
 static enum finish_epoch drbd_flush_after_epoch(struct drbd_connection *connection, struct drbd_epoch *epoch)
@@ -1608,7 +1606,7 @@ static void conn_wait_ee_empty(struct dr
 
 static int peer_request_fault_type(struct drbd_peer_request *peer_req)
 {
-	if (peer_req_op(peer_req) == REQ_OP_READ) {
+	if (!(peer_req->rw & REQ_WRITE)) {
 		return peer_req->flags & EE_APPLICATION ?
 			DRBD_FAULT_DT_RD : DRBD_FAULT_RS_RD;
 	} else {
@@ -1682,9 +1680,10 @@ next_bio:
 	 * should have been mapped to a "drbd protocol barrier".
 	 * REQ_OP_SECURE_ERASE: I don't see how we could ever support that.
 	 */
-	if (!(peer_req_op(peer_req) == REQ_OP_WRITE ||
-				peer_req_op(peer_req) == REQ_OP_READ)) {
-		drbd_err(device, "Invalid bio op received: 0x%x\n", peer_req->opf);
+	if (!((peer_req->rw & REQ_WRITE) ||
+				!(peer_req->rw & REQ_WRITE))) {
+		drbd_err(device, "Invalid bio op received: 0x%x\n",
+			 peer_req->rw);
 		err = -EINVAL;
 		goto fail;
 	}
@@ -1695,11 +1694,11 @@ next_bio:
 		goto fail;
 	}
 	/* > peer_req->i.sector, unless this is the first bio */
-	bio->bi_iter.bi_sector = sector;
-	bio_set_dev(bio, device->ldev->backing_bdev);
+	bio->bi_sector = sector;
+	bio->bi_bdev = device->ldev->backing_bdev;
 	/* we special case some flags in the multi-bio case, see below
 	 * (REQ_PREFLUSH, or BIO_RW_BARRIER in older kernels) */
-	bio->bi_opf = peer_req->opf;
+	bio->bi_rw = peer_req->rw;
 	bio->bi_private = peer_req;
 	bio->bi_end_io = drbd_peer_request_endio;
 
@@ -1711,7 +1710,7 @@ next_bio:
 		unsigned off, len;
 		int res;
 
-		if (peer_req_op(peer_req) == REQ_OP_READ) {
+		if (!(peer_req->rw & REQ_WRITE)) {
 			set_page_chain_offset(page, 0);
 			set_page_chain_size(page, min_t(unsigned, data_size, PAGE_SIZE));
 		}
@@ -1733,8 +1732,9 @@ next_bio:
 			if (bio->bi_vcnt == 0) {
 				drbd_err(device,
 					"bio_add_page(%p, %p, %u, %u): %d (bi_vcnt %u bi_max_vecs %u bi_sector %llu, bi_flags 0x%lx)\n",
-					bio, page, len, off, res, bio->bi_vcnt, bio->bi_max_vecs, (uint64_t)bio->bi_iter.bi_sector,
-					 (unsigned long)bio->bi_flags);
+					bio, page, len, off, res, bio->bi_vcnt, bio->bi_max_vecs,
+					(uint64_t) bio->bi_sector,
+					(unsigned long)bio->bi_flags);
 				err = -ENOSPC;
 				goto fail;
 			}
@@ -1761,7 +1761,7 @@ next_bio:
 		/* strip off REQ_PREFLUSH,
 		 * unless it is the first or last bio */
 		if (bios && bios->bi_next)
-			bios->bi_opf &= ~REQ_PREFLUSH;
+			bios->bi_rw &= ~REQ_FLUSH;
 	} while (bios);
 	return 0;
 
@@ -2069,8 +2069,8 @@ static int ignore_remaining_packet(struc
 static int recv_dless_read(struct drbd_peer_device *peer_device, struct drbd_request *req,
 			   sector_t sector, int data_size)
 {
-	struct bio_vec bvec;
-	struct bvec_iter iter;
+	struct bio_vec *bvec;
+	int iter;
 	struct bio *bio;
 	int digest_size, err, expect;
 	void *dig_in = peer_device->connection->int_dig_in;
@@ -2090,13 +2090,13 @@ static int recv_dless_read(struct drbd_p
 	peer_device->recv_cnt += data_size >> 9;
 
 	bio = req->master_bio;
-	D_ASSERT(peer_device->device, sector == bio->bi_iter.bi_sector);
+	D_ASSERT(peer_device->device, sector == bio->bi_sector);
 
 	bio_for_each_segment(bvec, bio, iter) {
-		void *mapped = kmap(bvec.bv_page) + bvec.bv_offset;
-		expect = min_t(int, data_size, bvec.bv_len);
+		void *mapped = kmap(bvec->bv_page) + bvec->bv_offset;
+		expect = min_t(int, data_size, bvec->bv_len);
 		err = drbd_recv_into(peer_device->connection, mapped, expect);
-		kunmap(bvec.bv_page);
+		kunmap(bvec->bv_page);
 		if (err)
 			return err;
 		data_size -= expect;
@@ -2167,7 +2167,7 @@ static int recv_resync_read(struct drbd_
 	 * respective _drbd_clear_done_ee */
 
 	peer_req->w.cb = e_end_resync_block;
-	peer_req->opf = REQ_OP_WRITE;
+	peer_req->rw = REQ_WRITE;
 	peer_req->submit_jif = jiffies;
 
 	spin_lock_irq(&device->resource->req_lock);
@@ -2592,28 +2592,20 @@ static int wait_for_and_update_peer_seq(
 	return ret;
 }
 
-static unsigned long wire_flags_to_bio_op(u32 dpf)
-{
-	if (dpf & DP_ZEROES)
-		return REQ_OP_WRITE_ZEROES;
-	if (dpf & DP_DISCARD)
-		return REQ_OP_DISCARD;
-	if (dpf & DP_WSAME)
-		return REQ_OP_WRITE_SAME;
-	else
-		return REQ_OP_WRITE;
-}
-
 /* see also bio_flags_to_wire() */
 static unsigned long wire_flags_to_bio(struct drbd_connection *connection, u32 dpf)
 {
-	unsigned long opf = wire_flags_to_bio_op(dpf) |
+	unsigned long opf = REQ_WRITE | (dpf & DP_DISCARD ? REQ_DISCARD : 0)
+#ifdef REQ_WRITE_SAME
+ | (dpf & DP_WSAME ? REQ_WRITE_SAME : 0)
+#endif
+ |
 		(dpf & DP_RW_SYNC ? REQ_SYNC : 0);
 
 	/* we used to communicate one bit only in older DRBD */
 	if (connection->agreed_pro_version >= 95)
 		opf |= (dpf & DP_FUA ? REQ_FUA : 0) |
-			    (dpf & DP_FLUSH ? REQ_PREFLUSH : 0);
+			    (dpf & DP_FLUSH ? REQ_FLUSH : 0);
 
 	return opf;
 }
@@ -2897,11 +2889,11 @@ static int receive_Data(struct drbd_conn
 	peer_req->submit_jif = jiffies;
 	peer_req->flags |= EE_APPLICATION;
 
-	peer_req->opf = wire_flags_to_bio(connection, d.dp_flags);
+	peer_req->rw = wire_flags_to_bio(connection, d.dp_flags);
 	if (pi->cmd == P_TRIM) {
 		D_ASSERT(peer_device, peer_req->i.size > 0);
 		D_ASSERT(peer_device, d.dp_flags & DP_DISCARD);
-		D_ASSERT(peer_device, peer_req_op(peer_req) == REQ_OP_DISCARD);
+		D_ASSERT(peer_device, (peer_req->rw & REQ_DISCARD));
 		D_ASSERT(peer_device, peer_req->page_chain.head == NULL);
 		D_ASSERT(peer_device, peer_req->page_chain.nr_pages == 0);
 		/* need to play safe: an older DRBD sender
@@ -2911,7 +2903,8 @@ static int receive_Data(struct drbd_conn
 	} else if (pi->cmd == P_ZEROES) {
 		D_ASSERT(peer_device, peer_req->i.size > 0);
 		D_ASSERT(peer_device, d.dp_flags & DP_ZEROES);
-		D_ASSERT(peer_device, peer_req_op(peer_req) == REQ_OP_WRITE_ZEROES);
+		D_ASSERT(peer_device,
+			 (false)/* WRITE_ZEROES not supported on this kernel */);
 		D_ASSERT(peer_device, peer_req->page_chain.head == NULL);
 		D_ASSERT(peer_device, peer_req->page_chain.nr_pages == 0);
 		/* Do (not) pass down BLKDEV_ZERO_NOUNMAP? */
@@ -2919,7 +2912,7 @@ static int receive_Data(struct drbd_conn
 			peer_req->flags |= EE_TRIM;
 	} else if (pi->cmd == P_WSAME) {
 		D_ASSERT(peer_device, peer_req->i.size > 0);
-		D_ASSERT(peer_device, peer_req_op(peer_req) == REQ_OP_WRITE_SAME);
+		D_ASSERT(peer_device, (peer_req->rw & REQ_WRITE_SAME));
 		D_ASSERT(peer_device, peer_req->page_chain.head != NULL);
 	} else if (peer_req->page_chain.head == NULL) {
 		/* Actually, this must not happen anymore,
@@ -2930,7 +2923,7 @@ static int receive_Data(struct drbd_conn
 		D_ASSERT(device, d.dp_flags & DP_FLUSH);
 	} else {
 		D_ASSERT(peer_device, peer_req->i.size > 0);
-		D_ASSERT(peer_device, peer_req_op(peer_req) == REQ_OP_WRITE);
+		D_ASSERT(peer_device, (peer_req->rw & REQ_WRITE));
 	}
 
 	if (d.dp_flags & DP_MAY_SET_IN_SYNC)
@@ -2952,14 +2945,14 @@ static int receive_Data(struct drbd_conn
 		epoch = list_entry(peer_req->epoch->list.prev, struct drbd_epoch, list);
 		if (epoch == peer_req->epoch) {
 			set_bit(DE_CONTAINS_A_BARRIER, &peer_req->epoch->flags);
-			peer_req->opf |= REQ_PREFLUSH | REQ_FUA;
+			peer_req->rw |= REQ_FLUSH | REQ_FUA;
 			peer_req->flags |= EE_IS_BARRIER;
 		} else {
 			if (atomic_read(&epoch->epoch_size) > 1 ||
 			    !test_bit(DE_CONTAINS_A_BARRIER, &epoch->flags)) {
 				set_bit(DE_BARRIER_IN_NEXT_EPOCH_ISSUED, &epoch->flags);
 				set_bit(DE_CONTAINS_A_BARRIER, &peer_req->epoch->flags);
-				peer_req->opf |= REQ_PREFLUSH | REQ_FUA;
+				peer_req->rw |= REQ_FLUSH | REQ_FUA;
 				peer_req->flags |= EE_IS_BARRIER;
 			}
 		}
@@ -3305,7 +3298,7 @@ static int receive_DataRequest(struct dr
 	peer_req->i.size = size;
 	peer_req->i.sector = sector;
 	peer_req->block_id = p->block_id;
-	peer_req->opf = REQ_OP_READ;
+	peer_req->rw =0;
 	/* no longer valid, about to call drbd_recv again for the digest... */
 	p = pi->data = NULL;
 
@@ -5950,9 +5943,9 @@ int abort_nested_twopc_work(struct drbd_
 	return 0;
 }
 
-void twopc_timer_fn(struct timer_list *t)
+void twopc_timer_fn(unsigned long data)
 {
-	struct drbd_resource *resource = from_timer(resource, t, twopc_timer);
+	struct drbd_resource *resource = (struct drbd_resource *)data;
 	unsigned long irq_flags;
 
 	spin_lock_irqsave(&resource->req_lock, irq_flags);
@@ -6208,9 +6201,9 @@ static int queued_twopc_work(struct drbd
 	return 0;
 }
 
-void queued_twopc_timer_fn(struct timer_list *t)
+void queued_twopc_timer_fn(unsigned long data)
 {
-	struct drbd_resource *resource = from_timer(resource, t, queued_twopc_timer);
+	struct drbd_resource *resource = (struct drbd_resource *)data;
 	struct queued_twopc *q;
 	unsigned long irq_flags;
 	unsigned long time = twopc_timeout(resource) / 4;
@@ -7762,7 +7755,7 @@ static int receive_rs_deallocated(struct
 		peer_req->i.sector = sector;
 		peer_req->block_id = ID_SYNCER;
 		peer_req->w.cb = e_end_resync_block;
-		peer_req->opf = REQ_OP_DISCARD;
+		peer_req->rw = REQ_DISCARD;
 		peer_req->submit_jif = jiffies;
 		peer_req->flags |= EE_TRIM;
 
@@ -7921,7 +7914,7 @@ static void cleanup_resync_leftovers(str
 	wake_up(&peer_device->device->misc_wait);
 
 	del_timer_sync(&peer_device->resync_timer);
-	resync_timer_fn(&peer_device->resync_timer);
+	resync_timer_fn((unsigned long)peer_device);
 	del_timer_sync(&peer_device->start_resync_timer);
 }
 
@@ -9150,7 +9143,7 @@ static void destroy_peer_ack_req(struct
 		container_of(kref, struct drbd_request, kref);
 
 	list_del(&req->tl_requests);
-	mempool_free(req, &drbd_request_mempool);
+	mempool_free(req, drbd_request_mempool);
 }
 
 static void cleanup_peer_ack_list(struct drbd_connection *connection)
--- drbd_main.c
+++ /tmp/cocci-output-201565-c5237b-drbd_main.c
@@ -55,7 +55,7 @@
 
 static int drbd_open(struct block_device *bdev, fmode_t mode);
 static void drbd_release(struct gendisk *gd, fmode_t mode);
-static void md_sync_timer_fn(struct timer_list *t);
+static void md_sync_timer_fn(unsigned long data);
 static int w_bitmap_io(struct drbd_work *w, int unused);
 static int flush_send_buffer(struct drbd_connection *connection, enum drbd_stream drbd_stream);
 static u64 __set_bitmap_slots(struct drbd_device *device, u64 bitmap_uuid, u64 do_nodes) __must_hold(local);
@@ -140,11 +140,11 @@ struct kmem_cache *drbd_request_cache;
 struct kmem_cache *drbd_ee_cache;	/* peer requests */
 struct kmem_cache *drbd_bm_ext_cache;	/* bitmap extents */
 struct kmem_cache *drbd_al_ext_cache;	/* activity log extents */
-mempool_t drbd_request_mempool;
-mempool_t drbd_ee_mempool;
-mempool_t drbd_md_io_page_pool;
-struct bio_set drbd_md_io_bio_set;
-struct bio_set drbd_io_bio_set;
+mempool_t *drbd_request_mempool;
+mempool_t *drbd_ee_mempool;
+mempool_t *drbd_md_io_page_pool;
+struct bio_set * drbd_md_io_bio_set;
+struct bio_set * drbd_io_bio_set;
 
 /* I do not use a standard mempool, because:
    1) I want to hand out the pre-allocated objects first.
@@ -165,10 +165,10 @@ static const struct block_device_operati
 
 struct bio *bio_alloc_drbd(gfp_t gfp_mask)
 {
-	if (!bioset_initialized(&drbd_md_io_bio_set))
+	if (!(drbd_md_io_bio_set != NULL))
 		return bio_alloc(gfp_mask, 1);
 
-	return bio_alloc_bioset(gfp_mask, 1, &drbd_md_io_bio_set);
+	return bio_alloc_bioset(gfp_mask, 1, drbd_md_io_bio_set);
 }
 
 #ifdef __CHECKER__
@@ -589,8 +589,8 @@ static int drbd_thread_setup(void *arg)
 	unsigned long flags;
 	int retval;
 
-	allow_kernel_signal(DRBD_SIGKILL);
-	allow_kernel_signal(SIGXCPU);
+	allow_signal(DRBD_SIGKILL);
+	allow_signal(SIGXCPU);
 
 	if (connection)
 		kref_get(&connection->kref);
@@ -2153,8 +2153,8 @@ static int _drbd_no_send_page(struct drb
 static int _drbd_send_bio(struct drbd_peer_device *peer_device, struct bio *bio)
 {
 	struct drbd_connection *connection = peer_device->connection;
-	struct bio_vec bvec;
-	struct bvec_iter iter;
+	struct bio_vec *bvec;
+	int iter;
 
 	/* Flush send buffer and make sure PAGE_SIZE is available... */
 	alloc_send_buffer(connection, PAGE_SIZE, DATA_STREAM);
@@ -2164,24 +2164,24 @@ static int _drbd_send_bio(struct drbd_pe
 	bio_for_each_segment(bvec, bio, iter) {
 		int err;
 
-		err = _drbd_no_send_page(peer_device, bvec.bv_page,
-					 bvec.bv_offset, bvec.bv_len,
-					 bio_iter_last(bvec, iter) ? 0 : MSG_MORE);
+		err = _drbd_no_send_page(peer_device, bvec->bv_page,
+					 bvec->bv_offset, bvec->bv_len,
+					 ((iter) == bio->bi_vcnt - 1) ? 0 : MSG_MORE);
 		if (err)
 			return err;
 		/* WRITE_SAME has only one segment */
-		if (bio_op(bio) == REQ_OP_WRITE_SAME)
+		if ((bio->bi_rw & REQ_WRITE_SAME))
 			break;
 
-		peer_device->send_cnt += bvec.bv_len >> 9;
+		peer_device->send_cnt += bvec->bv_len >> 9;
 	}
 	return 0;
 }
 
 static int _drbd_send_zc_bio(struct drbd_peer_device *peer_device, struct bio *bio)
 {
-	struct bio_vec bvec;
-	struct bvec_iter iter;
+	struct bio_vec *bvec;
+	int iter;
 	bool no_zc = drbd_disable_sendpage;
 
 	/* e.g. XFS meta- & log-data is in slab pages, which have a
@@ -2192,7 +2192,7 @@ static int _drbd_send_zc_bio(struct drbd
 	 * by someone, leading to some obscure delayed Oops somewhere else. */
 	if (!no_zc)
 		bio_for_each_segment(bvec, bio, iter) {
-			struct page *page = bvec.bv_page;
+			struct page *page = bvec->bv_page;
 
 			if (page_count(page) < 1 || PageSlab(page)) {
 				no_zc = true;
@@ -2212,7 +2212,7 @@ static int _drbd_send_zc_bio(struct drbd
 
 		err = tr_ops->send_zc_bio(transport, bio);
 		if (!err)
-			peer_device->send_cnt += bio->bi_iter.bi_size >> 9;
+			peer_device->send_cnt += bio->bi_size >> 9;
 
 		return err;
 	}
@@ -2248,19 +2248,19 @@ static int _drbd_send_zc_ee(struct drbd_
 static u32 bio_flags_to_wire(struct drbd_connection *connection, struct bio *bio)
 {
 	if (connection->agreed_pro_version >= 95)
-		return  (bio->bi_opf & REQ_SYNC ? DP_RW_SYNC : 0) |
-			(bio->bi_opf & REQ_FUA ? DP_FUA : 0) |
-			(bio->bi_opf & REQ_PREFLUSH ? DP_FLUSH : 0) |
-			(bio_op(bio) == REQ_OP_WRITE_SAME ? DP_WSAME : 0) |
-			(bio_op(bio) == REQ_OP_DISCARD ? DP_DISCARD : 0) |
-			(bio_op(bio) == REQ_OP_WRITE_ZEROES ?
+		return  (bio->bi_rw & REQ_SYNC ? DP_RW_SYNC : 0) |
+			(bio->bi_rw & REQ_FUA ? DP_FUA : 0) |
+			(bio->bi_rw & REQ_FLUSH ? DP_FLUSH : 0) |
+			((bio->bi_rw & REQ_WRITE_SAME) ? DP_WSAME : 0) |
+			((bio->bi_rw & REQ_DISCARD) ? DP_DISCARD : 0) |
+			((false)/* WRITE_ZEROES not supported on this kernel */ ?
 			  ((connection->agreed_features & DRBD_FF_WZEROES) ?
-			   (DP_ZEROES |(!(bio->bi_opf & REQ_NOUNMAP) ? DP_DISCARD : 0))
+			   (DP_ZEROES |(!(false)/* NOUNMAP not supported on this kernel */ ? DP_DISCARD : 0))
 			   : DP_DISCARD)
 			: 0);
 
 	/* else: we used to communicate one bit only in older DRBD */
-	return bio->bi_opf & REQ_SYNC ? DP_RW_SYNC : 0;
+	return bio->bi_rw & REQ_SYNC ? DP_RW_SYNC : 0;
 }
 
 /* Used to send write or TRIM aka REQ_OP_DISCARD requests
@@ -2279,9 +2279,8 @@ int drbd_send_dblock(struct drbd_peer_de
 	int digest_size = 0;
 	int err;
 	const unsigned s = drbd_req_state_by_peer_device(req, peer_device);
-	const int op = bio_op(req->master_bio);
 
-	if (op == REQ_OP_DISCARD || op == REQ_OP_WRITE_ZEROES) {
+	if ((req->master_bio->bi_rw & REQ_DISCARD) || (false)/* WRITE_ZEROES not supported on this kernel */) {
 		trim = drbd_prepare_command(peer_device, sizeof(*trim), DATA_STREAM);
 		if (!trim)
 			return -EIO;
@@ -2291,7 +2290,7 @@ int drbd_send_dblock(struct drbd_peer_de
 		if (peer_device->connection->integrity_tfm)
 			digest_size = crypto_shash_digestsize(peer_device->connection->integrity_tfm);
 
-		if (op == REQ_OP_WRITE_SAME) {
+		if ((req->master_bio->bi_rw & REQ_WRITE_SAME)) {
 			wsame = drbd_prepare_command(peer_device, sizeof(*wsame) + digest_size, DATA_STREAM);
 			if (!wsame)
 				return -EIO;
@@ -2334,7 +2333,7 @@ int drbd_send_dblock(struct drbd_peer_de
 
 	if (wsame) {
 		additional_size_command(peer_device->connection, DATA_STREAM,
-					bio_iovec(req->master_bio).bv_len);
+					bio_iovec(req->master_bio)->bv_len);
 		err = __send_command(peer_device->connection, device->vnr, P_WSAME, DATA_STREAM);
 	} else {
 		additional_size_command(peer_device->connection, DATA_STREAM, req->i.size);
@@ -2799,11 +2798,26 @@ static void drbd_destroy_mempools(void)
 
 	/* D_ASSERT(device, atomic_read(&drbd_pp_vacant)==0); */
 
-	bioset_exit(&drbd_io_bio_set);
-	bioset_exit(&drbd_md_io_bio_set);
-	mempool_exit(&drbd_md_io_page_pool);
-	mempool_exit(&drbd_ee_mempool);
-	mempool_exit(&drbd_request_mempool);
+	if (drbd_io_bio_set) {
+		bioset_free(drbd_io_bio_set);
+		drbd_io_bio_set = NULL;
+	}
+	if (drbd_md_io_bio_set) {
+		bioset_free(drbd_md_io_bio_set);
+		drbd_md_io_bio_set = NULL;
+	}
+	if (drbd_md_io_page_pool) {
+		mempool_destroy(drbd_md_io_page_pool);
+		drbd_md_io_page_pool = NULL;
+	}
+	if (drbd_ee_mempool) {
+		mempool_destroy(drbd_ee_mempool);
+		drbd_ee_mempool = NULL;
+	}
+	if (drbd_request_mempool) {
+		mempool_destroy(drbd_request_mempool);
+		drbd_request_mempool = NULL;
+	}
 	if (drbd_ee_cache)
 		kmem_cache_destroy(drbd_ee_cache);
 	if (drbd_request_cache)
@@ -2849,25 +2863,23 @@ static int drbd_create_mempools(void)
 		goto Enomem;
 
 	/* mempools */
-	ret = bioset_init(&drbd_io_bio_set, BIO_POOL_SIZE, 0, 0);
-	if (ret)
+	drbd_io_bio_set = bioset_create(BIO_POOL_SIZE, 0, 0);
+	if (drbd_io_bio_set == NULL)
 		goto Enomem;
 
-	ret = bioset_init(&drbd_md_io_bio_set, DRBD_MIN_POOL_PAGES, 0,
-			  BIOSET_NEED_BVECS);
-	if (ret)
+	drbd_md_io_bio_set = bioset_create(DRBD_MIN_POOL_PAGES, 0, 0);
+	if (drbd_md_io_bio_set == NULL)
 		goto Enomem;
 
-	ret = mempool_init_page_pool(&drbd_md_io_page_pool, DRBD_MIN_POOL_PAGES, 0);
+	ret = ((drbd_md_io_page_pool = mempool_create_page_pool(DRBD_MIN_POOL_PAGES, 0)) == NULL ? -ENOMEM : 0);
 	if (ret)
 		goto Enomem;
 
-	ret = mempool_init_slab_pool(&drbd_request_mempool, number,
-				     drbd_request_cache);
+	ret = ((drbd_request_mempool = mempool_create_slab_pool(number, drbd_request_cache)) == NULL ? -ENOMEM : 0);
 	if (ret)
 		goto Enomem;
 
-	ret = mempool_init_slab_pool(&drbd_ee_mempool, number, drbd_ee_cache);
+	ret = ((drbd_ee_mempool = mempool_create_slab_pool(number, drbd_ee_cache)) == NULL ? -ENOMEM : 0);
 	if (ret)
 		goto Enomem;
 
@@ -2980,7 +2992,7 @@ void drbd_reclaim_resource(struct rcu_he
 		kref_debug_put(&connection->kref_debug, 9);
 		kref_put(&connection->kref, drbd_destroy_connection);
 	}
-	mempool_free(resource->peer_ack_req, &drbd_request_mempool);
+	mempool_free(resource->peer_ack_req, drbd_request_mempool);
 	kref_debug_put(&resource->kref_debug, 8);
 	kref_put(&resource->kref, drbd_destroy_resource);
 }
@@ -3124,14 +3136,14 @@ static int drbd_congested(void *congeste
 	}
 
 	if (test_bit(CALLBACK_PENDING, &device->resource->flags)) {
-		r |= (1 << WB_async_congested);
+		r |= (1 << BDI_async_congested);
 		/* Without good local data, we would need to read from remote,
 		 * and that would need the worker thread as well, which is
 		 * currently blocked waiting for that usermode helper to
 		 * finish.
 		 */
 		if (!get_ldev_if_state(device, D_UP_TO_DATE))
-			r |= (1 << WB_sync_congested);
+			r |= (1 << BDI_sync_congested);
 		else
 			put_ldev(device);
 		r &= bdi_bits;
@@ -3144,13 +3156,13 @@ static int drbd_congested(void *congeste
 		put_ldev(device);
 	}
 
-	if (bdi_bits & (1 << WB_async_congested)) {
+	if (bdi_bits & (1 << BDI_async_congested)) {
 		struct drbd_peer_device *peer_device;
 
 		rcu_read_lock();
 		for_each_peer_device_rcu(peer_device, device) {
 			if (test_bit(NET_CONGESTED, &peer_device->connection->transport.flags)) {
-				r |= (1 << WB_async_congested);
+				r |= (1 << BDI_async_congested);
 				break;
 			}
 		}
@@ -3265,9 +3277,9 @@ void drbd_flush_peer_acks(struct drbd_re
 	spin_unlock_irq(&resource->req_lock);
 }
 
-static void peer_ack_timer_fn(struct timer_list *t)
+static void peer_ack_timer_fn(unsigned long data)
 {
-	struct drbd_resource *resource = from_timer(resource, t, peer_ack_timer);
+	struct drbd_resource *resource = (struct drbd_resource *)data;
 
 	drbd_flush_peer_acks(resource);
 }
@@ -3404,8 +3416,10 @@ struct drbd_resource *drbd_create_resour
 	INIT_LIST_HEAD(&resource->connections);
 	INIT_LIST_HEAD(&resource->transfer_log);
 	INIT_LIST_HEAD(&resource->peer_ack_list);
-	timer_setup(&resource->peer_ack_timer, peer_ack_timer_fn, 0);
-	timer_setup(&resource->repost_up_to_date_timer, repost_up_to_date_fn, 0);
+	setup_timer(&resource->peer_ack_timer, peer_ack_timer_fn,
+		    (unsigned long)resource);
+	setup_timer(&resource->repost_up_to_date_timer, repost_up_to_date_fn,
+		    (unsigned long)resource);
 	sema_init(&resource->state_sem, 1);
 	resource->role[NOW] = R_SECONDARY;
 	if (set_resource_options(resource, res_opts))
@@ -3422,11 +3436,13 @@ struct drbd_resource *drbd_create_resour
 	init_waitqueue_head(&resource->twopc_wait);
 	init_waitqueue_head(&resource->barrier_wait);
 	INIT_LIST_HEAD(&resource->twopc_parents);
-	timer_setup(&resource->twopc_timer, twopc_timer_fn, 0);
+	setup_timer(&resource->twopc_timer, twopc_timer_fn,
+		    (unsigned long)resource);
 	INIT_LIST_HEAD(&resource->twopc_work.list);
 	INIT_LIST_HEAD(&resource->queued_twopc);
 	spin_lock_init(&resource->queued_twopc_lock);
-	timer_setup(&resource->queued_twopc_timer, queued_twopc_timer_fn, 0);
+	setup_timer(&resource->queued_twopc_timer, queued_twopc_timer_fn,
+		    (unsigned long)resource);
 	drbd_init_workqueue(&resource->work);
 	drbd_thread_init(resource, &resource->worker, drbd_worker, "worker");
 	drbd_thread_start(&resource->worker);
@@ -3487,7 +3503,8 @@ struct drbd_connection *drbd_create_conn
 	mutex_init(&connection->mutex[CONTROL_STREAM]);
 
 	INIT_LIST_HEAD(&connection->connect_timer_work.list);
-	timer_setup(&connection->connect_timer, connect_timer_fn, 0);
+	setup_timer(&connection->connect_timer, connect_timer_fn,
+		    (unsigned long)connection);
 
 	drbd_thread_init(resource, &connection->receiver, drbd_receiver, "receiver");
 	connection->receiver.connection = connection;
@@ -3601,11 +3618,13 @@ struct drbd_peer_device *create_peer_dev
 		return NULL;
 	}
 
-	timer_setup(&peer_device->start_resync_timer, start_resync_timer_fn, 0);
+	setup_timer(&peer_device->start_resync_timer, start_resync_timer_fn,
+		    (unsigned long)peer_device);
 
 	INIT_LIST_HEAD(&peer_device->resync_work.list);
 	peer_device->resync_work.cb  = w_resync_timer;
-	timer_setup(&peer_device->resync_timer, resync_timer_fn, 0);
+	setup_timer(&peer_device->resync_timer, resync_timer_fn,
+		    (unsigned long)peer_device);
 
 	INIT_LIST_HEAD(&peer_device->propagate_uuids_work.list);
 	peer_device->propagate_uuids_work.cb = w_send_uuids;
@@ -3701,8 +3720,10 @@ enum drbd_ret_code drbd_create_device(st
 	spin_lock_init(&device->pending_bitmap_work.q_lock);
 	INIT_LIST_HEAD(&device->pending_bitmap_work.q);
 
-	timer_setup(&device->md_sync_timer, md_sync_timer_fn, 0);
-	timer_setup(&device->request_timer, request_timer_fn, 0);
+	setup_timer(&device->md_sync_timer, md_sync_timer_fn,
+		    (unsigned long)device);
+	setup_timer(&device->request_timer, request_timer_fn,
+		    (unsigned long)device);
 
 	init_waitqueue_head(&device->misc_wait);
 	init_waitqueue_head(&device->al_wait);
@@ -3710,7 +3731,7 @@ enum drbd_ret_code drbd_create_device(st
 
 	init_rwsem(&device->uuid_sem);
 
-	q = blk_alloc_queue(drbd_make_request, NUMA_NO_NODE);
+	q = blk_alloc_queue(GFP_KERNEL);
 	if (!q)
 		goto out_no_q;
 	device->rq_queue = q;
@@ -3734,7 +3755,9 @@ enum drbd_ret_code drbd_create_device(st
 
 	init_bdev_info(q->backing_dev_info, drbd_congested, device);
 
+	blk_queue_make_request(q, drbd_make_request);
 	blk_queue_write_cache(q, true, true);
+	blk_queue_merge_bvec(q, drbd_merge_bvec);
 
 	device->md_io.page = alloc_page(GFP_KERNEL);
 	if (!device->md_io.page)
@@ -4150,7 +4173,7 @@ int drbd_md_write(struct drbd_device *de
 	D_ASSERT(device, drbd_md_ss(device->ldev) == device->ldev->md.md_offset);
 	sector = device->ldev->md.md_offset;
 
-	err = drbd_md_sync_page_io(device, device->ldev, sector, REQ_OP_WRITE);
+	err = drbd_md_sync_page_io(device, device->ldev, sector, REQ_WRITE);
 	if (err) {
 		drbd_err(device, "meta data update failed!\n");
 		drbd_chk_io_error(device, err, DRBD_META_IO_ERROR);
@@ -4478,8 +4501,7 @@ int drbd_md_read(struct drbd_device *dev
 	if (!buffer)
 		return ERR_NOMEM;
 
-	if (drbd_md_sync_page_io(device, bdev, bdev->md.md_offset,
-				 REQ_OP_READ)) {
+	if (drbd_md_sync_page_io(device, bdev, bdev->md.md_offset, 0)) {
 		/* NOTE: can't do normal error processing here as this is
 		   called BEFORE disk is attached */
 		drbd_err(device, "Error while reading metadata.\n");
@@ -5621,9 +5643,9 @@ bool drbd_md_test_peer_flag(struct drbd_
 	return md->peers[peer_device->node_id].flags & flag;
 }
 
-static void md_sync_timer_fn(struct timer_list *t)
+static void md_sync_timer_fn(unsigned long data)
 {
-	struct drbd_device *device = from_timer(device, t, md_sync_timer);
+	struct drbd_device *device = (struct drbd_device *)data;
 	drbd_device_post_work(device, MD_SYNC);
 }
 
--- drbd_nla.c
+++ /tmp/cocci-output-201565-aeca64-drbd_nla.c
@@ -35,8 +35,7 @@ int drbd_nla_parse_nested(struct nlattr
 
 	err = drbd_nla_check_mandatory(maxtype, nla);
 	if (!err)
-		err = nla_parse_nested_deprecated(tb, maxtype, nla, policy,
-						  NULL);
+		err = nla_parse_nested(tb, maxtype, nla, policy, NULL);
 
 	return err;
 }
--- drbd_actlog.c
+++ /tmp/cocci-output-201565-e082a4-drbd_actlog.c
@@ -77,32 +77,32 @@ void wait_until_done_or_force_detached(s
 
 static int _drbd_md_sync_page_io(struct drbd_device *device,
 				 struct drbd_backing_dev *bdev,
-				 sector_t sector, int op)
+				 sector_t sector, int rw)
 {
 	struct bio *bio;
 	/* we do all our meta data IO in aligned 4k blocks. */
 	const int size = 4096;
-	int err, op_flags = 0;
+	int err;
 
-	if ((op == REQ_OP_WRITE) && !test_bit(MD_NO_FUA, &device->flags))
-		op_flags |= REQ_FUA | REQ_PREFLUSH;
-	op_flags |= REQ_META | REQ_SYNC | REQ_PRIO;
+	if ((rw & REQ_WRITE) && !test_bit(MD_NO_FUA, &device->flags))
+		rw |= REQ_FUA | REQ_FLUSH;
+	rw |=REQ_NOIDLE | REQ_META | REQ_SYNC | REQ_PRIO;
 
 	device->md_io.done = 0;
 	device->md_io.error = -ENODEV;
 
 	bio = bio_alloc_drbd(GFP_NOIO);
-	bio_set_dev(bio, bdev->md_bdev);
-	bio->bi_iter.bi_sector = sector;
+	bio->bi_bdev = bdev->md_bdev;
+	bio->bi_sector = sector;
 	err = -EIO;
 	if (bio_add_page(bio, device->md_io.page, size, 0) != size)
 		goto out;
 	bio->bi_private = device;
 	bio->bi_end_io = drbd_md_endio;
 
-	bio->bi_opf = op | op_flags;
+	bio->bi_rw = rw;
 
-	if (op != REQ_OP_WRITE && device->disk_state[NOW] == D_DISKLESS && device->ldev == NULL)
+	if (!(rw & REQ_WRITE) && device->disk_state[NOW] == D_DISKLESS && device->ldev == NULL)
 		/* special case, drbd_md_read() during drbd_adm_attach(): no get_ldev */
 		;
 	else if (!get_ldev_if_state(device, D_ATTACHING)) {
@@ -115,11 +115,11 @@ static int _drbd_md_sync_page_io(struct
 	bio_get(bio); /* one bio_put() is in the completion handler */
 	atomic_inc(&device->md_io.in_use); /* drbd_md_put_buffer() is in the completion handler */
 	device->md_io.submit_jif = jiffies;
-	if (drbd_insert_fault(device, (op == REQ_OP_WRITE) ? DRBD_FAULT_MD_WR : DRBD_FAULT_MD_RD)) {
-		bio->bi_status = BLK_STS_IOERR;
-		bio_endio(bio);
+	if (drbd_insert_fault(device, (rw & REQ_WRITE) ? DRBD_FAULT_MD_WR : DRBD_FAULT_MD_RD)) {
+		bio_endio(bio,
+			  (10 == 0 ? 0 : 10 == 9 ? -ENOMEM : 10 == 1 ? -EOPNOTSUPP : -EIO));
 	} else {
-		submit_bio(bio);
+		submit_bio(rw, bio);
 	}
 	wait_until_done_or_force_detached(device, bdev, &device->md_io.done);
 	err = device->md_io.error;
@@ -129,7 +129,7 @@ static int _drbd_md_sync_page_io(struct
 }
 
 int drbd_md_sync_page_io(struct drbd_device *device, struct drbd_backing_dev *bdev,
-			 sector_t sector, int op)
+			 sector_t sector, int rw)
 {
 	int err;
 	D_ASSERT(device, atomic_read(&device->md_io.in_use) == 1);
@@ -142,7 +142,7 @@ int drbd_md_sync_page_io(struct drbd_dev
 
 	drbd_dbg(device, "meta_data io: %s [%d]:%s(,%llus,%s) %pS\n",
 	     current->comm, current->pid, __func__,
-	     (unsigned long long)sector, (op == REQ_OP_WRITE) ? "WRITE" : "READ",
+	     (unsigned long long)sector, (rw & REQ_WRITE) ? "WRITE" : "READ",
 	     (void*)_RET_IP_ );
 
 	if (sector < drbd_md_first_sector(bdev) ||
@@ -150,13 +150,13 @@ int drbd_md_sync_page_io(struct drbd_dev
 		drbd_alert(device, "%s [%d]:%s(,%llus,%s) out of range md access!\n",
 		     current->comm, current->pid, __func__,
 		     (unsigned long long)sector,
-		     (op == REQ_OP_WRITE) ? "WRITE" : "READ");
+		     (rw & REQ_WRITE) ? "WRITE" : "READ");
 
-	err = _drbd_md_sync_page_io(device, bdev, sector, op);
+	err = _drbd_md_sync_page_io(device, bdev, sector, rw);
 	if (err) {
 		drbd_err(device, "drbd_md_sync_page_io(,%llus,%s) failed with error %d\n",
 		    (unsigned long long)sector,
-		    (op == REQ_OP_WRITE) ? "WRITE" : "READ", err);
+		    (rw & REQ_WRITE) ? "WRITE" : "READ", err);
 	}
 	return err;
 }
@@ -435,7 +435,7 @@ static int __al_write_transaction(struct
 		rcu_read_unlock();
 		if (write_al_updates) {
 			ktime_aggregate_delta(device, start_kt, al_mid_kt);
-			if (drbd_md_sync_page_io(device, device->ldev, sector, REQ_OP_WRITE)) {
+			if (drbd_md_sync_page_io(device, device->ldev, sector, REQ_WRITE)) {
 				err = -EIO;
 				drbd_chk_io_error(device, 1, DRBD_META_IO_ERROR);
 			} else {
--- kref_debug.c
+++ /tmp/cocci-output-201565-125f12-kref_debug.c
@@ -108,7 +108,7 @@ void print_kref_debug_info(struct seq_fi
 		char obj_name[80];
 
 		debug_refs = number_of_debug_refs(debug_info);
-		refs = refcount_read(&debug_info->kref->refcount);
+		refs = atomic_read(&debug_info->kref->refcount);
 		debug_info->class->get_object_name(debug_info, obj_name);
 
 		seq_printf(seq, "class: %s, name: %s, refs: %d, dr: %d\n",
--- drbd_nl.c
+++ /tmp/cocci-output-201565-ef23b5-drbd_nl.c
@@ -111,7 +111,7 @@ static int drbd_msg_put_info(struct sk_b
 	if (!info || !info[0])
 		return 0;
 
-	nla = nla_nest_start_noflag(skb, DRBD_NLA_CFG_REPLY);
+	nla = nla_nest_start(skb, DRBD_NLA_CFG_REPLY);
 	if (!nla)
 		return err;
 
@@ -138,7 +138,7 @@ static int drbd_msg_sprintf_info(struct
 	int aligned_len;
 	char *msg_buf;
 
-	nla = nla_nest_start_noflag(skb, DRBD_NLA_CFG_REPLY);
+	nla = nla_nest_start(skb, DRBD_NLA_CFG_REPLY);
 	if (!nla)
 		return err;
 
@@ -1271,15 +1271,16 @@ static void opener_info(struct drbd_reso
 	mutex_lock(&resource->open_release);
 
 	idr_for_each_entry(&resource->devices, device, i) {
-		struct timespec64 ts;
+		struct timespec ts;
 		struct tm tm;
 
 		o = list_first_entry_or_null(&device->openers.list, struct opener, list);
 		if (!o)
 			continue;
 
-		ts = ktime_to_timespec64(o->opened);
-		time64_to_tm(ts.tv_sec, -sys_tz.tz_minuteswest * 60, &tm);
+		ts = ktime_to_timespec(o->opened);
+		time_to_tm((time_t)ts.tv_sec, -sys_tz.tz_minuteswest * 60,
+			   &tm);
 
 		drbd_msg_sprintf_info(reply_skb,
 				      "/dev/drbd%d opened by %s (pid %d) "
@@ -1988,9 +1989,19 @@ static void decide_on_discard_support(st
 		 * topology on all peers. */
 		blk_queue_discard_granularity(q, 512);
 		q->limits.max_discard_sectors = drbd_max_discard_sectors(device->resource);
-		blk_queue_flag_set(QUEUE_FLAG_DISCARD, q);
+		{
+			unsigned long ____flags0;
+			spin_lock_irqsave(q->queue_lock, ____flags0);
+			queue_flag_set(QUEUE_FLAG_DISCARD, q);
+			spin_unlock_irqrestore(q->queue_lock, ____flags0);
+		}
 	} else {
-		blk_queue_flag_clear(QUEUE_FLAG_DISCARD, q);
+		{
+			unsigned long ____flags1;
+			spin_lock_irqsave(q->queue_lock, ____flags1);
+			queue_flag_clear(QUEUE_FLAG_DISCARD, q);
+			spin_unlock_irqrestore(q->queue_lock, ____flags1);
+		}
 		blk_queue_discard_granularity(q, 0);
 		q->limits.max_discard_sectors = 0;
 	}
@@ -2008,21 +2019,6 @@ static void fixup_discard_if_not_support
 	}
 }
 
-static void fixup_write_zeroes(struct drbd_device *device, struct request_queue *q)
-{
-	/* Fixup max_write_zeroes_sectors after blk_queue_stack_limits():
-	 * if we can handle "zeroes" efficiently on the protocol,
-	 * we want to do that, even if our backend does not announce
-	 * max_write_zeroes_sectors itself. */
-
-	/* If all peers announce WZEROES support, use it.  Otherwise, rather
-	 * send explicit zeroes than rely on some discard-zeroes-data magic. */
-	if (common_connection_features(device->resource) & DRBD_FF_WZEROES)
-		q->limits.max_write_zeroes_sectors = DRBD_MAX_BBIO_SECTORS;
-	else
-		q->limits.max_write_zeroes_sectors = 0;
-}
-
 static void decide_on_write_same_support(struct drbd_device *device,
 			struct request_queue *q,
 			struct request_queue *b, struct o_qlim *o,
@@ -2120,7 +2116,6 @@ static void drbd_setup_queue_param(struc
 		adjust_ra_pages(q, b);
 	}
 	fixup_discard_if_not_supported(q);
-	fixup_write_zeroes(device, q);
 }
 
 void drbd_reconsider_queue_parameters(struct drbd_device *device, struct drbd_backing_dev *bdev, struct o_qlim *o)
@@ -4970,7 +4965,7 @@ static int nla_put_drbd_cfg_context(stru
 				    struct drbd_path *path)
 {
 	struct nlattr *nla;
-	nla = nla_nest_start_noflag(skb, DRBD_NLA_CFG_CONTEXT);
+	nla = nla_nest_start(skb, DRBD_NLA_CFG_CONTEXT);
 	if (!nla)
 		goto nla_put_failure;
 	if (device)
@@ -5100,8 +5095,8 @@ static void device_to_statistics(struct
 		q = bdev_get_queue(device->ldev->backing_bdev);
 		s->dev_lower_blocked =
 			bdi_congested(q->backing_dev_info,
-				      (1 << WB_async_congested) |
-				      (1 << WB_sync_congested));
+				      (1 << BDI_async_congested) |
+				      (1 << BDI_sync_congested));
 		put_ldev(device);
 	}
 	s->dev_size = drbd_get_capacity(device->this_bdev);
@@ -5227,7 +5222,7 @@ int drbd_adm_dump_connections_done(struc
 static int connection_paths_to_skb(struct sk_buff *skb, struct drbd_connection *connection)
 {
 	struct drbd_path *path;
-	struct nlattr *tla = nla_nest_start_noflag(skb, DRBD_NLA_PATH_PARMS);
+	struct nlattr *tla = nla_nest_start(skb, DRBD_NLA_PATH_PARMS);
 	if (!tla)
 		goto nla_put_failure;
 
--- drbd_req.c
+++ /tmp/cocci-output-201565-b68b22-drbd_req.c
@@ -22,11 +22,49 @@
 
 static bool drbd_may_do_local_read(struct drbd_device *device, sector_t sector, int size);
 
+/* ATTENTION: this is a compat implementation of generic_*_io_acct,
+ * added by a coccinelle patch.
+ * it is more likely to be broken than the upstream version is.
+ */
+static inline void generic_start_io_acct(struct request_queue *q, int rw,
+					 unsigned long sects,
+					 struct hd_struct *part){
+	int cpu;
+
+	cpu = part_stat_lock();
+	part_round_stats(cpu, part);
+	part_stat_inc(cpu, part, ios[rw]);
+	part_stat_add(cpu, part, sectors[rw], sects);
+	(void)cpu;/* The macro invocations above want the cpu argument, I do not like
+		       the compiler warning about cpu only assigned but never used... */
+	/* part_inc_in_flight(part, rw); */
+	{
+		BUILD_BUG_ON(sizeof(atomic_t) != sizeof(part->in_flight[0]));
+	}
+	atomic_inc((atomic_t *)&part->in_flight[rw]);
+	part_stat_unlock();
+}
+
+static inline void generic_end_io_acct(struct request_queue *q, int rw,
+				       struct hd_struct *part,
+				       unsigned long start_time)
+{
+	unsigned long duration = jiffies - start_time;
+	int cpu;
+
+	cpu = part_stat_lock();
+	part_stat_add(cpu, part, ticks[rw], duration);
+	part_round_stats(cpu, part);
+	/* part_dec_in_flight(part, rw); */
+	atomic_dec((atomic_t *)&part->in_flight[rw]);
+	part_stat_unlock();
+}
+
 static struct drbd_request *drbd_req_new(struct drbd_device *device, struct bio *bio_src)
 {
 	struct drbd_request *req;
 
-	req = mempool_alloc(&drbd_request_mempool, GFP_NOIO);
+	req = mempool_alloc(drbd_request_mempool, GFP_NOIO);
 	if (!req)
 		return NULL;
 
@@ -40,8 +78,8 @@ static struct drbd_request *drbd_req_new
 	req->epoch = 0;
 
 	drbd_clear_interval(&req->i);
-	req->i.sector = bio_src->bi_iter.bi_sector;
-	req->i.size = bio_src->bi_iter.bi_size;
+	req->i.sector = bio_src->bi_sector;
+	req->i.size = bio_src->bi_size;
 	req->i.local = true;
 	req->i.waiting = false;
 
@@ -55,9 +93,9 @@ static struct drbd_request *drbd_req_new
 	kref_init(&req->kref);
 
 	req->local_rq_state = (bio_data_dir(bio_src) == WRITE ? RQ_WRITE : 0)
-	              | (bio_op(bio_src) == REQ_OP_WRITE_SAME ? RQ_WSAME : 0)
-	              | (bio_op(bio_src) == REQ_OP_WRITE_ZEROES ? RQ_ZEROES : 0)
-	              | (bio_op(bio_src) == REQ_OP_DISCARD ? RQ_UNMAP : 0);
+	              | ((bio_src->bi_rw & REQ_WRITE_SAME) ? RQ_WSAME : 0)
+	              | ((false)/* WRITE_ZEROES not supported on this kernel */ ? RQ_ZEROES : 0)
+	              | ((bio_src->bi_rw & REQ_DISCARD) ? RQ_UNMAP : 0);
 
 	return req;
 }
@@ -65,7 +103,7 @@ static struct drbd_request *drbd_req_new
 static void req_destroy_no_send_peer_ack(struct kref *kref)
 {
 	struct drbd_request *req = container_of(kref, struct drbd_request, kref);
-	mempool_free(req, &drbd_request_mempool);
+	mempool_free(req, drbd_request_mempool);
 }
 
 void drbd_queue_peer_ack(struct drbd_resource *resource, struct drbd_request *req)
@@ -73,7 +111,7 @@ void drbd_queue_peer_ack(struct drbd_res
 	struct drbd_connection *connection;
 	bool queued = false;
 
-	refcount_set(&req->kref.refcount, 1); /* was 0, instead of kref_get() */
+	atomic_set(&req->kref.refcount, 1); /* was 0, instead of kref_get() */
 	rcu_read_lock();
 	for_each_connection_rcu(connection, resource) {
 		unsigned int node_id = connection->peer_node_id;
@@ -270,7 +308,8 @@ void drbd_req_destroy(struct kref *kref)
 				drbd_queue_peer_ack(resource, peer_ack_req);
 				peer_ack_req = NULL;
 			} else
-				mempool_free(peer_ack_req, &drbd_request_mempool);
+				mempool_free(peer_ack_req,
+					     drbd_request_mempool);
 		}
 		req->device = NULL;
 		resource->peer_ack_req = req;
@@ -280,7 +319,7 @@ void drbd_req_destroy(struct kref *kref)
 		if (!peer_ack_req)
 			resource->last_peer_acked_dagtag = req->dagtag_sector;
 	} else
-		mempool_free(req, &drbd_request_mempool);
+		mempool_free(req, drbd_request_mempool);
 
 	/* In both branches of the if above, the reference to device gets released */
 	kref_debug_put(&device->kref_debug, 6);
@@ -293,7 +332,7 @@ void drbd_req_destroy(struct kref *kref)
 	 */
 	if (destroy_next) {
 		req = destroy_next;
-		if (refcount_dec_and_test(&req->kref.refcount))
+		if (atomic_dec_and_test(&req->kref.refcount))
 			goto tail_recursion;
 	}
 }
@@ -326,8 +365,8 @@ void complete_master_bio(struct drbd_dev
 		struct bio_and_error *m)
 {
 	int rw = bio_data_dir(m->bio);
-	m->bio->bi_status = errno_to_blk_status(m->error);
-	bio_endio(m->bio);
+	bio_endio(m->bio,
+		  ((m->error == 0 ? 0 : m->error == -ENOMEM ? 9 : m->error == -EOPNOTSUPP ? 1 : 10) == 0 ? 0 : (m->error == 0 ? 0 : m->error == -ENOMEM ? 9 : m->error == -EOPNOTSUPP ? 1 : 10) == 9 ? -ENOMEM : (m->error == 0 ? 0 : m->error == -ENOMEM ? 9 : m->error == -EOPNOTSUPP ? 1 : 10) == 1 ? -EOPNOTSUPP : -EIO));
 	dec_ap_bio(device, rw);
 }
 
@@ -416,7 +455,9 @@ void drbd_req_complete(struct drbd_reque
 		start_new_tl_epoch(device->resource);
 
 	/* Update disk stats */
-	bio_end_io_acct(req->master_bio, req->start_jif);
+	generic_end_io_acct(req->device->rq_queue,
+			    bio_data_dir(req->master_bio),
+			    &req->device->vdisk->part0, req->start_jif);
 
 	/* If READ failed,
 	 * have it be pushed back to the retry work queue,
@@ -433,8 +474,8 @@ void drbd_req_complete(struct drbd_reque
 	 * WRITE should have used all available paths already.
 	 */
 	if (!ok &&
-	    bio_op(req->master_bio) == REQ_OP_READ &&
-	    !(req->master_bio->bi_opf & REQ_RAHEAD) &&
+	    !(req->master_bio->bi_rw & REQ_WRITE) &&
+	    !(req->master_bio->bi_rw & REQ_RAHEAD) &&
 	    !list_empty(&req->tl_requests))
 		req->local_rq_state |= RQ_POSTPONED;
 
@@ -1446,8 +1487,8 @@ static void drbd_process_discard_or_zero
 {
 	int err = drbd_issue_discard_or_zero_out(req->device,
 				req->i.sector, req->i.size >> 9, flags);
-	req->private_bio->bi_status = err ? BLK_STS_IOERR : BLK_STS_OK;
-	bio_endio(req->private_bio);
+	bio_endio(req->private_bio,
+		  ((err ? 10 : 0) == 0 ? 0 : (err ? 10 : 0) == 9 ? -ENOMEM : (err ? 10 : 0) == 1 ? -EOPNOTSUPP : -EIO));
 }
 
 static void
@@ -1457,14 +1498,14 @@ drbd_submit_req_private_bio(struct drbd_
 	struct bio *bio = req->private_bio;
 	unsigned int type;
 
-	if (bio_op(bio) != REQ_OP_READ)
+	if ((bio->bi_rw & REQ_WRITE))
 		type = DRBD_FAULT_DT_WR;
-	else if (bio->bi_opf & REQ_RAHEAD)
+	else if (bio->bi_rw & REQ_RAHEAD)
 		type = DRBD_FAULT_DT_RA;
 	else
 		type = DRBD_FAULT_DT_RD;
 
-	bio_set_dev(bio, device->ldev->backing_bdev);
+	bio->bi_bdev = device->ldev->backing_bdev;
 
 	/* State may have changed since we grabbed our reference on the
 	 * device->ldev member. Double check, and short-circuit to endio.
@@ -1473,20 +1514,20 @@ drbd_submit_req_private_bio(struct drbd_
 	 * this bio. */
 	if (get_ldev(device)) {
 		if (drbd_insert_fault(device, type)) {
-			bio->bi_status = BLK_STS_IOERR;
-			bio_endio(bio);
-		} else if (bio_op(bio) == REQ_OP_WRITE_ZEROES) {
+			bio_endio(bio,
+				  (10 == 0 ? 0 : 10 == 9 ? -ENOMEM : 10 == 1 ? -EOPNOTSUPP : -EIO));
+		} else if ((false)/* WRITE_ZEROES not supported on this kernel */) {
 			drbd_process_discard_or_zeroes_req(req, EE_ZEROOUT |
-			    ((bio->bi_opf & REQ_NOUNMAP) ? 0 : EE_TRIM));
-		} else if (bio_op(bio) == REQ_OP_DISCARD) {
+			    ((false)/* NOUNMAP not supported on this kernel */ ? 0 : EE_TRIM));
+		} else if ((bio->bi_rw & REQ_DISCARD)) {
 			drbd_process_discard_or_zeroes_req(req, EE_TRIM);
 		} else {
 			generic_make_request(bio);
 		}
 		put_ldev(device);
 	} else {
-		bio->bi_status = BLK_STS_IOERR;
-		bio_endio(bio);
+		bio_endio(bio,
+			  (10 == 0 ? 0 : 10 == 9 ? -ENOMEM : 10 == 1 ? -EOPNOTSUPP : -EIO));
 	}
  }
 
@@ -1507,7 +1548,7 @@ static void drbd_queue_write(struct drbd
 static void req_make_private_bio(struct drbd_request *req, struct bio *bio_src)
 {
 	struct bio *bio;
-	bio = bio_clone_fast(bio_src, GFP_NOIO, &drbd_io_bio_set);
+	bio = bio_clone(bio_src, GFP_NOIO);
 
 	req->private_bio = bio;
 
@@ -1546,13 +1587,16 @@ drbd_request_prepare(struct drbd_device
 		/* only pass the error to the upper layers.
 		 * if user cannot handle io errors, that's not our business. */
 		drbd_err(device, "could not kmalloc() req\n");
-		bio->bi_status = BLK_STS_RESOURCE;
-		bio_endio(bio);
+		bio_endio(bio,
+			  (9 == 0 ? 0 : 9 == 9 ? -ENOMEM : 9 == 1 ? -EOPNOTSUPP : -EIO));
 		return ERR_PTR(-ENOMEM);
 	}
 
 	/* Update disk stats */
-	req->start_jif = bio_start_io_acct(req->master_bio);
+	req->start_jif = start_jif;
+	generic_start_io_acct(req->device->rq_queue,
+			      bio_data_dir(req->master_bio), req->i.size >> 9,
+			      &req->device->vdisk->part0);
 
 	if (get_ldev(device))
 		req_make_private_bio(req, bio);
@@ -1573,8 +1617,8 @@ drbd_request_prepare(struct drbd_device
 		atomic_add(interval_to_al_extents(&req->i), &device->wait_for_actlog_ecnt);
 
 	/* process discards always from our submitter thread */
-	if ((bio_op(bio) == REQ_OP_WRITE_ZEROES) ||
-	    (bio_op(bio) == REQ_OP_DISCARD))
+	if ((false)/* WRITE_ZEROES not supported on this kernel */ ||
+	    (bio->bi_rw & REQ_DISCARD))
 		goto queue_for_submitter_thread;
 
 	if (req->private_bio && !test_bit(AL_SUSPENDED, &device->flags)) {
@@ -1751,7 +1795,8 @@ static void drbd_send_and_submit(struct
 		 * replicating, in which case there is no point. */
 		if (unlikely(req->i.size == 0)) {
 			/* The only size==0 bios we expect are empty flushes. */
-			D_ASSERT(device, req->master_bio->bi_opf & REQ_PREFLUSH);
+			D_ASSERT(device,
+				 req->master_bio->bi_rw & REQ_FLUSH);
 			_req_mod(req, QUEUE_AS_DRBD_BARRIER, NULL);
 		} else if (!drbd_process_write_request(req))
 			no_remote = true;
@@ -2179,7 +2224,7 @@ void do_submit(struct work_struct *ws)
 	}
 }
 
-blk_qc_t drbd_make_request(struct request_queue *q, struct bio *bio)
+void drbd_make_request(struct request_queue *q, struct bio *bio)
 {
 	struct drbd_device *device = (struct drbd_device *) q->queuedata;
 #ifdef CONFIG_DRBD_TIMING_STATS
@@ -2187,12 +2232,10 @@ blk_qc_t drbd_make_request(struct reques
 #endif
 	unsigned long start_jif;
 
-	blk_queue_split(q, &bio);
-
 	if (device->cached_err_io) {
-		bio->bi_status = BLK_STS_IOERR;
-		bio_endio(bio);
-		return BLK_QC_T_NONE;
+		bio_endio(bio,
+			  (10 == 0 ? 0 : 10 == 9 ? -ENOMEM : 10 == 1 ? -EOPNOTSUPP : -EIO));
+		return;
 	}
 
 	ktime_get_accounting(start_kt);
@@ -2200,7 +2243,40 @@ blk_qc_t drbd_make_request(struct reques
 
 	__drbd_make_request(device, bio, start_kt, start_jif);
 
-	return BLK_QC_T_NONE;
+	return;
+}
+
+/* This is called by bio_add_page().
+ *
+ * q->max_hw_sectors and other global limits are already enforced there.
+ *
+ * We need to call down to our lower level device,
+ * in case it has special restrictions.
+ *
+ * As long as the BIO is empty we have to allow at least one bvec,
+ * regardless of size and offset, so no need to ask lower levels.
+ */
+int drbd_merge_bvec(struct request_queue *q, struct bvec_merge_data *bvm,
+		    struct bio_vec *bvec)
+{
+	struct drbd_device *device = (struct drbd_device *)q->queuedata;
+	unsigned int bio_size = bvm->bi_size;
+	int limit = DRBD_MAX_BIO_SIZE;
+	int backing_limit;
+
+	if (bio_size && get_ldev(device)) {
+		unsigned int max_hw_sectors = queue_max_hw_sectors(q);
+		struct request_queue * const b = device->ldev->backing_bdev->bd_disk->queue;
+		if (b->merge_bvec_fn) {
+			bvm->bi_bdev = device->ldev->backing_bdev;
+			backing_limit = b->merge_bvec_fn(b, bvm, bvec);
+			limit = min(limit, backing_limit);
+		}
+		put_ldev(device);
+		if ((limit >> 9) > max_hw_sectors)
+			limit = max_hw_sectors << 9;
+	}
+	return limit;
 }
 
 static unsigned long time_min_in_future(unsigned long now,
@@ -2286,9 +2362,9 @@ static bool net_timeout_reached(struct d
  * to expire twice (worst case) to become effective. Good enough.
  */
 
-void request_timer_fn(struct timer_list *t)
+void request_timer_fn(unsigned long data)
 {
-	struct drbd_device *device = from_timer(device, t, request_timer);
+	struct drbd_device *device = (struct drbd_device *)data;
 	struct drbd_connection *connection;
 	struct drbd_request *req_read, *req_write;
 	unsigned long oldest_submit_jif;
--- drbd_debugfs.c
+++ /tmp/cocci-output-201565-b727dd-drbd_debugfs.c
@@ -594,12 +594,12 @@ static int drbd_single_open(struct file
 	if (!parent || !parent->d_inode)
 		goto out;
 	/* serialize with d_delete() */
-	inode_lock(d_inode(parent));
+	mutex_lock(&parent->d_inode->i_mutex);
 	/* Make sure the object is still alive */
 	if (simple_positive(file->f_path.dentry)
 	&& kref_get_unless_zero(kref))
 		ret = 0;
-	inode_unlock(d_inode(parent));
+	mutex_unlock(&parent->d_inode->i_mutex);
 	if (!ret) {
 		ret = single_open(file, show, data);
 		if (ret)
@@ -1299,7 +1299,7 @@ static int drbd_single_open_peer_device(
 	parent = file->f_path.dentry->d_parent;
 	if (!parent || !parent->d_inode)
 		goto out;
-	inode_lock(d_inode(parent));
+	mutex_lock(&parent->d_inode->i_mutex);
 	if (!simple_positive(file->f_path.dentry))
 		goto out_unlock;
 
@@ -1308,7 +1308,7 @@ static int drbd_single_open_peer_device(
 
 	if (got_connection && got_device) {
 		int ret;
-		inode_unlock(d_inode(parent));
+		mutex_unlock(&parent->d_inode->i_mutex);
 		ret = single_open(file, show, peer_device);
 		if (ret) {
 			kref_put(&connection->kref, drbd_destroy_connection);
@@ -1322,7 +1322,7 @@ static int drbd_single_open_peer_device(
 	if (got_device)
 		kref_put(&device->kref, drbd_destroy_device);
 out_unlock:
-	inode_unlock(d_inode(parent));
+	mutex_unlock(&parent->d_inode->i_mutex);
 out:
 	return -ESTALE;
 }
@@ -1716,6 +1716,45 @@ static const struct file_operations drbd
 
 static int drbd_compat_show(struct seq_file *m, void *ignored)
 {
+	seq_puts(m, "timer_setup__no_present\n");
+	seq_puts(m, "bio_bi_bdev__yes_present\n");
+	seq_puts(m, "refcount_inc__no_present\n");
+	seq_puts(m, "struct_bvec_iter__no_present\n");
+	seq_puts(m, "rdma_create_id__no_has_net_ns\n");
+	seq_puts(m, "ib_device__no_has_ops\n");
+	seq_puts(m, "ib_alloc_pd__no_has_2_params\n");
+	seq_puts(m, "ib_post__no_const\n");
+	seq_puts(m, "blk_queue_make_request__yes_present\n");
+	seq_puts(m, "make_request__no_is_blk_qc_t__yes_is_void\n");
+	seq_puts(m, "bio__no_bi_status__no_bi_error\n");
+	seq_puts(m, "bio__no_bi_status\n");
+	seq_puts(m, "kernel_read__yes_before_4_13\n");
+	seq_puts(m, "sock_ops__no_returns_addr_len\n");
+	seq_puts(m, "sock_create_kern__no_has_five_parameters\n");
+	seq_puts(m, "wb_congested_enum__no_present\n");
+	seq_puts(m, "time64_to_tm__no_present\n");
+	seq_puts(m, "ktime_to_timespec64__no_present\n");
+	seq_puts(m, "d_inode__no_present\n");
+	seq_puts(m, "inode_lock__no_present\n");
+	seq_puts(m, "bioset_init__no_present\n");
+	seq_puts(m, "bioset_init__no_present__no_bio_clone_fast\n");
+	seq_puts(m, "bioset_init__no_present__no_need_bvecs\n");
+	seq_puts(m, "genl_policy__yes_in_ops\n");
+	seq_puts(m, "blk_queue_merge_bvec__yes_present\n");
+	seq_puts(m, "blk_queue_split__no_present\n");
+	seq_puts(m, "blk_queue_flag_set__no_present\n");
+	seq_puts(m, "req_noidle__yes_present\n");
+	seq_puts(m, "req_nounmap__no_present\n");
+	seq_puts(m, "write_zeroes__no_capable\n");
+	seq_puts(m, "bio_bi_opf__no_present\n");
+	seq_puts(m, "bio_start_io_acct__no_present\n");
+	seq_puts(m, "generic_start_io_acct__no_present\n");
+	seq_puts(m, "req_write__yes_present\n");
+	seq_puts(m, "nla_nest_start_noflag__no_present\n");
+	seq_puts(m, "nla_parse_deprecated__no_present\n");
+	seq_puts(m, "allow_kernel_signal__no_present\n");
+	seq_puts(m, "struct_size__no_present\n");
+	seq_puts(m, "part_stat_h__no_present\n");
 	return 0;
 }
 
--- drbd-headers/linux/genl_magic_func.h
+++ drbd-headers/linux/genl_magic_func.h
@@ -233,6 +233,7 @@ static const char *CONCAT_(GENL_MAGIC_FAMILY, _genl_cmd_to_str)(__u8 cmd)
 {								\
 	handler							\
 	.cmd = op_name,						\
+	.policy	= CONCAT_(GENL_MAGIC_FAMILY, _tla_nl_policy),	\
 },
 
 #define ZZZ_genl_ops		CONCAT_(GENL_MAGIC_FAMILY, _genl_ops)
@@ -291,7 +292,6 @@ static struct genl_family ZZZ_genl_family __read_mostly = {
 #ifdef COMPAT_HAVE_GENL_FAMILY_PARALLEL_OPS
 	.parallel_ops = true,
 #endif
-	.policy = CONCAT_(GENL_MAGIC_FAMILY, _tla_nl_policy),
 };
 
 /*
